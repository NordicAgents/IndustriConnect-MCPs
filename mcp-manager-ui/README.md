# LLM Chat UI

A modern, intuitive chat interface for working with local and thirdâ€‘party language models in the IndustriConnect ecosystem. Built with React, TypeScript, and Tailwind CSS.

## Features

- ğŸ¨ **Beautiful UI** â€“ Modern interface inspired by Claude Code with clean design  
- ğŸŒ“ **Dark/Light Mode** â€“ Seamless theme switching with system preference detection  
- ğŸ”Œ **MCP Server Integration** â€“ Connect and manage Model Context Protocol servers (MQTT, OPC UA, etc.)
- â˜ï¸ **Cloud LLM Support** â€“ Talk to OpenAI (ChatGPT), Google Gemini, and Anthropic Claude  
- ğŸ’» **Local LLM Support (Ollama)** â€“ Chat with local models running via Ollama  
- ğŸ’¬ **Interactive Chat** â€“ Streamingâ€‘style conversational UI with copyâ€‘toâ€‘clipboard  
- ğŸ“ **Session Management** â€“ Simple session list to keep track of conversations  
- ğŸ’¾ **Local Storage** â€“ Remembers chat backend and LLM configuration between visits

## Getting Started

### Prerequisites
- Node.js (v18 or higher)
- npm or yarn
- Python (for running MCP servers like MQTT/OPC UA)
- `uv` (Python package manager)

### Installation

1. Install dependencies (including backend):
   ```bash
   npm install
   cd mcp-backend && npm install && cd ..
   ```

### Running the Application

Start both the frontend UI and the backend service with a single command:

```bash
npm run dev
```

This will start:
- Frontend UI at http://localhost:5173 (or similar)
- Backend WebSocket server at ws://localhost:3003
- Backend HTTP server at http://localhost:3002

## Features

- **Chat Interface**: Interact with Cloud LLMs (OpenAI, Gemini, Anthropic) or local Ollama models.
- **MCP Server Integration**: Configure and connect to real MCP servers (MQTT, OPC UA, etc.).
- **Tool Calling**: LLMs can automatically discover and use tools provided by connected MCP servers.
- **Real-time Updates**: WebSocket connection ensures live status updates from MCP servers.

## Usage

1. **Configure MCP Servers** (New!)
   - Click "Configure Servers" in the MCP Servers section of the sidebar
   - Option 1: Use the form to add servers manually
   - Option 2: Import a Cursor-style JSON configuration file
   - Option 3: Use JSON editor mode to paste configuration directly
   - Example configuration available in `mcp-config-example.json`

2. **Connect to MCP Servers**
   - Start your MCP server processes externally (e.g., run the MQTT or OPC UA servers)
   - In the sidebar, click "Connect" next to each configured server
   - View available tools by expanding the server entry
   - Connected servers will show a green indicator

3. **Choose a Chat Backend**
   - In the top bar of the chat panel, select:
     - `Cloud LLM (ChatGPT / Gemini / Claude)` or  
     - `Local Ollama`

4. **Configure Cloud LLMs**
   - Select a provider (OpenAI, Gemini, Anthropic)
   - Pick a model from the dropdown
   - Provide an API key either:
     - via `.env` file (`VITE_OPENAI_API_KEY`, `VITE_GEMINI_API_KEY`, `VITE_ANTHROPIC_API_KEY`), or  
     - directly in the UI when prompted

5. **Configure Ollama**
   - Ensure Ollama is running locally (default: `http://localhost:11434`)
   - Select or refresh the model list in the header

6. **Start Chatting**
   - Type your message in the input box
   - Press `Enter` to send, `Shift+Enter` for a new line
   - Click the copy icon on assistant messages to copy responses

7. **Manage Sessions**
   - Sessions are created automatically when you start chatting
   - Use the sidebar to switch between sessions

## Project Structure

```
ui/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/        # React components
â”‚   â”‚   â”œâ”€â”€ Sidebar.tsx    # Left sidebar with sessions and theme toggle
â”‚   â”‚   â”œâ”€â”€ ChatPanel.tsx  # Right panel for chat interface
â”‚   â”‚   â””â”€â”€ ThemeProvider.tsx   # Theme context provider
â”‚   â”œâ”€â”€ types.ts           # TypeScript type definitions
â”‚   â”œâ”€â”€ utils/             # Utility functions
â”‚   â”‚   â”œâ”€â”€ theme.ts       # Theme management
â”‚   â”‚   â”œâ”€â”€ storage.ts     # LocalStorage helpers
â”‚   â”‚   â””â”€â”€ llm.ts         # Cloud/Ollama LLM helpers
â”‚   â”œâ”€â”€ App.tsx            # Main application component
â”‚   â””â”€â”€ main.tsx           # Entry point
â”œâ”€â”€ index.html
â”œâ”€â”€ package.json
â””â”€â”€ vite.config.ts
```

## Future Enhancements

- [ ] Streaming responses
- [ ] Per-session message history persistence
- [ ] Command/Prompt templates
- [ ] Multi-tab support for multiple sessions

## License

ISC
