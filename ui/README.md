# LLM Chat UI

A modern, intuitive chat interface for working with local and thirdâ€‘party language models in the IndustriConnect ecosystem. Built with React, TypeScript, and Tailwind CSS.

## Features

- ğŸ¨ **Beautiful UI** â€“ Modern interface inspired by Claude Code with clean design  
- ğŸŒ“ **Dark/Light Mode** â€“ Seamless theme switching with system preference detection  
- â˜ï¸ **Cloud LLM Support** â€“ Talk to OpenAI (ChatGPT), Google Gemini, and Anthropic Claude  
- ğŸ’» **Local LLM Support (Ollama)** â€“ Chat with local models running via Ollama  
- ğŸ’¬ **Interactive Chat** â€“ Streamingâ€‘style conversational UI with copyâ€‘toâ€‘clipboard  
- ğŸ“ **Session Management** â€“ Simple session list to keep track of conversations  
- ğŸ’¾ **Local Storage** â€“ Remembers chat backend and LLM configuration between visits

## Getting Started

### Prerequisites

- Node.js 18+ and npm

### Installation

```bash
cd ui
npm install
```

### Development

```bash
npm run dev
```

The application will open at `http://localhost:3000`

### Build

```bash
npm run build
```

### Preview Production Build

```bash
npm run preview
```

## Usage

1. **Choose a Chat Backend**
   - In the top bar of the chat panel, select:
     - `Cloud LLM (ChatGPT / Gemini / Claude)` or  
     - `Local Ollama`

2. **Configure Cloud LLMs**
   - Select a provider (OpenAI, Gemini, Anthropic)
   - Pick a model from the dropdown
   - Provide an API key either:
     - via `.env` file (`VITE_OPENAI_API_KEY`, `VITE_GEMINI_API_KEY`, `VITE_ANTHROPIC_API_KEY`), or  
     - directly in the UI when prompted

3. **Configure Ollama**
   - Ensure Ollama is running locally (default: `http://localhost:11434`)
   - Select or refresh the model list in the header

4. **Start Chatting**
   - Type your message in the input box
   - Press `Enter` to send, `Shift+Enter` for a new line
   - Click the copy icon on assistant messages to copy responses

5. **Manage Sessions**
   - Sessions are created automatically when you start chatting
   - Use the sidebar to switch between sessions

## Project Structure

```
ui/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/        # React components
â”‚   â”‚   â”œâ”€â”€ Sidebar.tsx    # Left sidebar with sessions and theme toggle
â”‚   â”‚   â”œâ”€â”€ ChatPanel.tsx  # Right panel for chat interface
â”‚   â”‚   â””â”€â”€ ThemeProvider.tsx   # Theme context provider
â”‚   â”œâ”€â”€ types.ts           # TypeScript type definitions
â”‚   â”œâ”€â”€ utils/             # Utility functions
â”‚   â”‚   â”œâ”€â”€ theme.ts       # Theme management
â”‚   â”‚   â”œâ”€â”€ storage.ts     # LocalStorage helpers
â”‚   â”‚   â””â”€â”€ llm.ts         # Cloud/Ollama LLM helpers
â”‚   â”œâ”€â”€ App.tsx            # Main application component
â”‚   â””â”€â”€ main.tsx           # Entry point
â”œâ”€â”€ index.html
â”œâ”€â”€ package.json
â””â”€â”€ vite.config.ts
```

## Future Enhancements

- [ ] Streaming responses
- [ ] Per-session message history persistence
- [ ] Command/Prompt templates
- [ ] Multi-tab support for multiple sessions

## License

ISC
